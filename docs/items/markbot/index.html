<!doctype html><html lang=en-us><head><meta http-equiv=content-type content="text/html" charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge,chrome=1"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="The personal website of Will Vesey, with projects, blog posts, and contact information"><meta name=keywords content="Will,William,Vesey,website,projects,personal,github,resume,blog,tech"><title>vesey.tech</title><style>*,::before,::after{box-sizing:border-box;border-width:0;border-style:solid;border-color:#e5e7eb}::before,::after{--tw-content:''}html{line-height:1.5;-webkit-text-size-adjust:100%;-moz-tab-size:4;-o-tab-size:4;tab-size:4;font-family:ui-sans-serif,system-ui,-apple-system,BlinkMacSystemFont,segoe ui,Roboto,helvetica neue,Arial,noto sans,sans-serif,apple color emoji,segoe ui emoji,segoe ui symbol,noto color emoji}body{margin:0;line-height:inherit}hr{height:0;color:inherit;border-top-width:1px}abbr:where([title]){-webkit-text-decoration:underline dotted;text-decoration:underline dotted}h1,h2,h3,h4,h5,h6{font-size:inherit;font-weight:inherit}a{color:inherit;text-decoration:inherit}b,strong{font-weight:bolder}code,kbd,samp,pre{font-family:JetBrains Mono,ui-monospace,SFMono-Regular,Menlo,Monaco,Consolas,liberation mono,courier new,monospace;font-size:1em}small{font-size:80%}sub,sup{font-size:75%;line-height:0;position:relative;vertical-align:baseline}sub{bottom:-.25em}sup{top:-.5em}table{text-indent:0;border-color:inherit;border-collapse:collapse}button,input,optgroup,select,textarea{font-family:inherit;font-size:100%;line-height:inherit;color:inherit;margin:0;padding:0}button,select{text-transform:none}button,[type=button],[type=reset],[type=submit]{-webkit-appearance:button;background-color:transparent;background-image:none}:-moz-focusring{outline:auto}:-moz-ui-invalid{box-shadow:none}progress{vertical-align:baseline}::-webkit-inner-spin-button,::-webkit-outer-spin-button{height:auto}[type=search]{-webkit-appearance:textfield;outline-offset:-2px}::-webkit-search-decoration{-webkit-appearance:none}::-webkit-file-upload-button{-webkit-appearance:button;font:inherit}summary{display:list-item}blockquote,dl,dd,h1,h2,h3,h4,h5,h6,hr,figure,p,pre{margin:0}fieldset{margin:0;padding:0}legend{padding:0}ol,ul,menu{list-style:none;margin:0;padding:0}textarea{resize:vertical}input::-moz-placeholder,textarea::-moz-placeholder{opacity:1;color:#9ca3af}input:-ms-input-placeholder,textarea:-ms-input-placeholder{opacity:1;color:#9ca3af}input::placeholder,textarea::placeholder{opacity:1;color:#9ca3af}button,[role=button]{cursor:pointer}:disabled{cursor:default}img,svg,video,canvas,audio,iframe,embed,object{display:block;vertical-align:middle}img,video{max-width:100%;height:auto}[hidden]{display:none}*,::before,::after{--tw-translate-x:0;--tw-translate-y:0;--tw-rotate:0;--tw-skew-x:0;--tw-skew-y:0;--tw-scale-x:1;--tw-scale-y:1;--tw-pan-x: ;--tw-pan-y: ;--tw-pinch-zoom: ;--tw-scroll-snap-strictness:proximity;--tw-ordinal: ;--tw-slashed-zero: ;--tw-numeric-figure: ;--tw-numeric-spacing: ;--tw-numeric-fraction: ;--tw-ring-inset: ;--tw-ring-offset-width:0px;--tw-ring-offset-color:#fff;--tw-ring-color:rgb(59 130 246 / 0.5);--tw-ring-offset-shadow:0 0 #0000;--tw-ring-shadow:0 0 #0000;--tw-shadow:0 0 #0000;--tw-shadow-colored:0 0 #0000;--tw-blur: ;--tw-brightness: ;--tw-contrast: ;--tw-grayscale: ;--tw-hue-rotate: ;--tw-invert: ;--tw-saturate: ;--tw-sepia: ;--tw-drop-shadow: ;--tw-backdrop-blur: ;--tw-backdrop-brightness: ;--tw-backdrop-contrast: ;--tw-backdrop-grayscale: ;--tw-backdrop-hue-rotate: ;--tw-backdrop-invert: ;--tw-backdrop-opacity: ;--tw-backdrop-saturate: ;--tw-backdrop-sepia: }.container{width:100%}@media(min-width:640px){.container{max-width:640px}}@media(min-width:768px){.container{max-width:768px}}@media(min-width:1024px){.container{max-width:1024px}}@media(min-width:1280px){.container{max-width:1280px}}@media(min-width:1536px){.container{max-width:1536px}}.prose{color:var(--tw-prose-body);max-width:65ch}.prose :where([class~=lead]):not(:where([class~=not-prose] *)){color:var(--tw-prose-lead);font-size:1.25em;line-height:1.6;margin-top:1.2em;margin-bottom:1.2em}.prose :where(a):not(:where([class~=not-prose] *)){color:#f9faf9;text-decoration:underline;font-weight:500}.prose :where(strong):not(:where([class~=not-prose] *)){color:#eee8d5;font-weight:600}.prose :where(ol):not(:where([class~=not-prose] *)){list-style-type:decimal;padding-left:1.625em}.prose :where(ol[type=A]):not(:where([class~=not-prose] *)){list-style-type:upper-alpha}.prose :where(ol[type=a]):not(:where([class~=not-prose] *)){list-style-type:lower-alpha}.prose :where(ol[type=As]):not(:where([class~=not-prose] *)){list-style-type:upper-alpha}.prose :where(ol[type=as]):not(:where([class~=not-prose] *)){list-style-type:lower-alpha}.prose :where(ol[type=I]):not(:where([class~=not-prose] *)){list-style-type:upper-roman}.prose :where(ol[type=i]):not(:where([class~=not-prose] *)){list-style-type:lower-roman}.prose :where(ol[type=Is]):not(:where([class~=not-prose] *)){list-style-type:upper-roman}.prose :where(ol[type=is]):not(:where([class~=not-prose] *)){list-style-type:lower-roman}.prose :where(ol[type="1"]):not(:where([class~=not-prose] *)){list-style-type:decimal}.prose :where(ul):not(:where([class~=not-prose] *)){list-style-type:disc;padding-left:1.625em}.prose :where(ol>li):not(:where([class~=not-prose] *))::marker{font-weight:400;color:var(--tw-prose-counters)}.prose :where(ul>li):not(:where([class~=not-prose] *))::marker{color:var(--tw-prose-bullets)}.prose :where(hr):not(:where([class~=not-prose] *)){border-color:var(--tw-prose-hr);border-top-width:1px;margin-top:3em;margin-bottom:3em}.prose :where(blockquote):not(:where([class~=not-prose] *)){font-weight:500;font-style:italic;color:var(--tw-prose-quotes);border-left-width:.25rem;border-left-color:var(--tw-prose-quote-borders);quotes:"\201C""\201D""\2018""\2019";margin-top:1.6em;margin-bottom:1.6em;padding-left:1em}.prose :where(blockquote p:first-of-type):not(:where([class~=not-prose] *))::before{content:open-quote}.prose :where(blockquote p:last-of-type):not(:where([class~=not-prose] *))::after{content:close-quote}.prose :where(h1):not(:where([class~=not-prose] *)){color:#f9faf9;font-weight:800;font-size:2.25em;margin-top:0;margin-bottom:.8888889em;line-height:1.1111111}.prose :where(h1 strong):not(:where([class~=not-prose] *)){font-weight:900}.prose :where(h2):not(:where([class~=not-prose] *)){color:#f9faf9;font-weight:700;font-size:1.5em;margin-top:2em;margin-bottom:1em;line-height:1.3333333}.prose :where(h2 strong):not(:where([class~=not-prose] *)){font-weight:800}.prose :where(h3):not(:where([class~=not-prose] *)){color:#f9faf9;font-weight:600;font-size:1.25em;margin-top:1.6em;margin-bottom:.6em;line-height:1.6}.prose :where(h3 strong):not(:where([class~=not-prose] *)){font-weight:700}.prose :where(h4):not(:where([class~=not-prose] *)){color:#f9faf9;font-weight:600;margin-top:1.5em;margin-bottom:.5em;line-height:1.5}.prose :where(h4 strong):not(:where([class~=not-prose] *)){font-weight:700}.prose :where(figure>*):not(:where([class~=not-prose] *)){margin-top:0;margin-bottom:0}.prose :where(figcaption):not(:where([class~=not-prose] *)){color:var(--tw-prose-captions);font-size:.875em;line-height:1.4285714;margin-top:.8571429em}.prose :where(code):not(:where([class~=not-prose] *)){color:#f9faf9;font-weight:600;font-size:.875em}.prose :where(code):not(:where([class~=not-prose] *))::before{content:"`"}.prose :where(code):not(:where([class~=not-prose] *))::after{content:"`"}.prose :where(a code):not(:where([class~=not-prose] *)){color:var(--tw-prose-links)}.prose :where(pre):not(:where([class~=not-prose] *)){color:var(--tw-prose-pre-code);background-color:#222d31;overflow-x:auto;font-weight:400;font-size:.875em;line-height:1.7142857;margin-top:1.7142857em;margin-bottom:1.7142857em;border-radius:.375rem;padding-top:.8571429em;padding-right:1.1428571em;padding-bottom:.8571429em;padding-left:1.1428571em}.prose :where(pre code):not(:where([class~=not-prose] *)){background-color:transparent;border-width:0;border-radius:0;padding:0;font-weight:inherit;color:inherit;font-size:inherit;font-family:inherit;line-height:inherit}.prose :where(pre code):not(:where([class~=not-prose] *))::before{content:none}.prose :where(pre code):not(:where([class~=not-prose] *))::after{content:none}.prose :where(table):not(:where([class~=not-prose] *)){width:100%;table-layout:auto;text-align:left;margin-top:2em;margin-bottom:2em;font-size:.875em;line-height:1.7142857}.prose :where(thead):not(:where([class~=not-prose] *)){border-bottom-width:1px;border-bottom-color:var(--tw-prose-th-borders);color:#f9faf9}.prose :where(thead th):not(:where([class~=not-prose] *)){color:var(--tw-prose-headings);font-weight:600;vertical-align:bottom;padding-right:.5714286em;padding-bottom:.5714286em;padding-left:.5714286em}.prose :where(tbody tr):not(:where([class~=not-prose] *)){border-bottom-width:1px;border-bottom-color:var(--tw-prose-td-borders)}.prose :where(tbody tr:last-child):not(:where([class~=not-prose] *)){border-bottom-width:0}.prose :where(tbody td):not(:where([class~=not-prose] *)){vertical-align:baseline;padding-top:.5714286em;padding-right:.5714286em;padding-bottom:.5714286em;padding-left:.5714286em}.prose{--tw-prose-body:#374151;--tw-prose-headings:#111827;--tw-prose-lead:#4b5563;--tw-prose-links:#111827;--tw-prose-bold:#111827;--tw-prose-counters:#6b7280;--tw-prose-bullets:#d1d5db;--tw-prose-hr:#e5e7eb;--tw-prose-quotes:#111827;--tw-prose-quote-borders:#e5e7eb;--tw-prose-captions:#6b7280;--tw-prose-code:#111827;--tw-prose-pre-code:#e5e7eb;--tw-prose-pre-bg:#1f2937;--tw-prose-th-borders:#d1d5db;--tw-prose-td-borders:#e5e7eb;--tw-prose-invert-body:#d1d5db;--tw-prose-invert-headings:#fff;--tw-prose-invert-lead:#9ca3af;--tw-prose-invert-links:#fff;--tw-prose-invert-bold:#fff;--tw-prose-invert-counters:#9ca3af;--tw-prose-invert-bullets:#4b5563;--tw-prose-invert-hr:#374151;--tw-prose-invert-quotes:#f3f4f6;--tw-prose-invert-quote-borders:#374151;--tw-prose-invert-captions:#9ca3af;--tw-prose-invert-code:#fff;--tw-prose-invert-pre-code:#d1d5db;--tw-prose-invert-pre-bg:rgb(0 0 0 / 50%);--tw-prose-invert-th-borders:#4b5563;--tw-prose-invert-td-borders:#374151;font-size:1rem;line-height:1.75}.prose :where(p):not(:where([class~=not-prose] *)){margin-top:1.25em;margin-bottom:1.25em}.prose :where(p):not(:where([class~=not-prose] *)) code{background-color:#353836}.prose :where(img):not(:where([class~=not-prose] *)){margin-top:2em;margin-bottom:2em}.prose :where(video):not(:where([class~=not-prose] *)){margin-top:2em;margin-bottom:2em}.prose :where(figure):not(:where([class~=not-prose] *)){margin-top:2em;margin-bottom:2em}.prose :where(h2 code):not(:where([class~=not-prose] *)){font-size:.875em}.prose :where(h3 code):not(:where([class~=not-prose] *)){font-size:.9em}.prose :where(li):not(:where([class~=not-prose] *)){margin-top:.5em;margin-bottom:.5em}.prose :where(ol>li):not(:where([class~=not-prose] *)){padding-left:.375em}.prose :where(ul>li):not(:where([class~=not-prose] *)){padding-left:.375em}.prose>:where(ul>li p):not(:where([class~=not-prose] *)){margin-top:.75em;margin-bottom:.75em}.prose>:where(ul>li>*:first-child):not(:where([class~=not-prose] *)){margin-top:1.25em}.prose>:where(ul>li>*:last-child):not(:where([class~=not-prose] *)){margin-bottom:1.25em}.prose>:where(ol>li>*:first-child):not(:where([class~=not-prose] *)){margin-top:1.25em}.prose>:where(ol>li>*:last-child):not(:where([class~=not-prose] *)){margin-bottom:1.25em}.prose :where(ul ul,ul ol,ol ul,ol ol):not(:where([class~=not-prose] *)){margin-top:.75em;margin-bottom:.75em}.prose :where(hr+*):not(:where([class~=not-prose] *)){margin-top:0}.prose :where(h2+*):not(:where([class~=not-prose] *)){margin-top:0}.prose :where(h3+*):not(:where([class~=not-prose] *)){margin-top:0}.prose :where(h4+*):not(:where([class~=not-prose] *)){margin-top:0}.prose :where(thead th:first-child):not(:where([class~=not-prose] *)){padding-left:0}.prose :where(thead th:last-child):not(:where([class~=not-prose] *)){padding-right:0}.prose :where(tbody td:first-child):not(:where([class~=not-prose] *)){padding-left:0}.prose :where(tbody td:last-child):not(:where([class~=not-prose] *)){padding-right:0}.prose>:where(:first-child):not(:where([class~=not-prose] *)){margin-top:0}.prose>:where(:last-child):not(:where([class~=not-prose] *)){margin-bottom:0}.prose :where(h5):not(:where([class~=not-prose] *)){color:#f9faf9}.prose :where(h6):not(:where([class~=not-prose] *)){color:#f9faf9}.prose :where(div):not(:where([class~=not-prose] *)) pre{background-color:#222d31!important}.prose-lg{font-size:1.125rem;line-height:1.7777778}.prose-lg :where(p):not(:where([class~=not-prose] *)){margin-top:1.3333333em;margin-bottom:1.3333333em}.prose-lg :where([class~=lead]):not(:where([class~=not-prose] *)){font-size:1.2222222em;line-height:1.4545455;margin-top:1.0909091em;margin-bottom:1.0909091em}.prose-lg :where(blockquote):not(:where([class~=not-prose] *)){margin-top:1.6666667em;margin-bottom:1.6666667em;padding-left:1em}.prose-lg :where(h1):not(:where([class~=not-prose] *)){font-size:2.6666667em;margin-top:0;margin-bottom:.8333333em;line-height:1}.prose-lg :where(h2):not(:where([class~=not-prose] *)){font-size:1.6666667em;margin-top:1.8666667em;margin-bottom:1.0666667em;line-height:1.3333333}.prose-lg :where(h3):not(:where([class~=not-prose] *)){font-size:1.3333333em;margin-top:1.6666667em;margin-bottom:.6666667em;line-height:1.5}.prose-lg :where(h4):not(:where([class~=not-prose] *)){margin-top:1.7777778em;margin-bottom:.4444444em;line-height:1.5555556}.prose-lg :where(img):not(:where([class~=not-prose] *)){margin-top:1.7777778em;margin-bottom:1.7777778em}.prose-lg :where(video):not(:where([class~=not-prose] *)){margin-top:1.7777778em;margin-bottom:1.7777778em}.prose-lg :where(figure):not(:where([class~=not-prose] *)){margin-top:1.7777778em;margin-bottom:1.7777778em}.prose-lg :where(figure>*):not(:where([class~=not-prose] *)){margin-top:0;margin-bottom:0}.prose-lg :where(figcaption):not(:where([class~=not-prose] *)){font-size:.8888889em;line-height:1.5;margin-top:1em}.prose-lg :where(code):not(:where([class~=not-prose] *)){font-size:.8888889em}.prose-lg :where(h2 code):not(:where([class~=not-prose] *)){font-size:.8666667em}.prose-lg :where(h3 code):not(:where([class~=not-prose] *)){font-size:.875em}.prose-lg :where(pre):not(:where([class~=not-prose] *)){font-size:.8888889em;line-height:1.75;margin-top:2em;margin-bottom:2em;border-radius:.375rem;padding-top:1em;padding-right:1.5em;padding-bottom:1em;padding-left:1.5em}.prose-lg :where(ol):not(:where([class~=not-prose] *)){padding-left:1.5555556em}.prose-lg :where(ul):not(:where([class~=not-prose] *)){padding-left:1.5555556em}.prose-lg :where(li):not(:where([class~=not-prose] *)){margin-top:.6666667em;margin-bottom:.6666667em}.prose-lg :where(ol>li):not(:where([class~=not-prose] *)){padding-left:.4444444em}.prose-lg :where(ul>li):not(:where([class~=not-prose] *)){padding-left:.4444444em}.prose-lg>:where(ul>li p):not(:where([class~=not-prose] *)){margin-top:.8888889em;margin-bottom:.8888889em}.prose-lg>:where(ul>li>*:first-child):not(:where([class~=not-prose] *)){margin-top:1.3333333em}.prose-lg>:where(ul>li>*:last-child):not(:where([class~=not-prose] *)){margin-bottom:1.3333333em}.prose-lg>:where(ol>li>*:first-child):not(:where([class~=not-prose] *)){margin-top:1.3333333em}.prose-lg>:where(ol>li>*:last-child):not(:where([class~=not-prose] *)){margin-bottom:1.3333333em}.prose-lg :where(ul ul,ul ol,ol ul,ol ol):not(:where([class~=not-prose] *)){margin-top:.8888889em;margin-bottom:.8888889em}.prose-lg :where(hr):not(:where([class~=not-prose] *)){margin-top:3.1111111em;margin-bottom:3.1111111em}.prose-lg :where(hr+*):not(:where([class~=not-prose] *)){margin-top:0}.prose-lg :where(h2+*):not(:where([class~=not-prose] *)){margin-top:0}.prose-lg :where(h3+*):not(:where([class~=not-prose] *)){margin-top:0}.prose-lg :where(h4+*):not(:where([class~=not-prose] *)){margin-top:0}.prose-lg :where(table):not(:where([class~=not-prose] *)){font-size:.8888889em;line-height:1.5}.prose-lg :where(thead th):not(:where([class~=not-prose] *)){padding-right:.75em;padding-bottom:.75em;padding-left:.75em}.prose-lg :where(thead th:first-child):not(:where([class~=not-prose] *)){padding-left:0}.prose-lg :where(thead th:last-child):not(:where([class~=not-prose] *)){padding-right:0}.prose-lg :where(tbody td):not(:where([class~=not-prose] *)){padding-top:.75em;padding-right:.75em;padding-bottom:.75em;padding-left:.75em}.prose-lg :where(tbody td:first-child):not(:where([class~=not-prose] *)){padding-left:0}.prose-lg :where(tbody td:last-child):not(:where([class~=not-prose] *)){padding-right:0}.prose-lg>:where(:first-child):not(:where([class~=not-prose] *)){margin-top:0}.prose-lg>:where(:last-child):not(:where([class~=not-prose] *)){margin-bottom:0}.absolute{position:absolute}.relative{position:relative}.sticky{position:-webkit-sticky;position:sticky}.inset-0{top:0;right:0;bottom:0;left:0}.top-0{top:0}.right-0{right:0}.my-8{margin-top:2rem;margin-bottom:2rem}.mt-4{margin-top:1rem}.mr-2{margin-right:.5rem}.mb-2{margin-bottom:.5rem}.block{display:block}.flex{display:flex}.grid{display:grid}.h-screen{height:100vh}.h-full{height:100%}.h-10{height:2.5rem}.h-4{height:1rem}.w-screen{width:100vw}.w-full{width:100%}.w-4{width:1rem}.max-w-none{max-width:none}.max-w-full{max-width:100%}.flex-1{flex:1}.flex-2{flex:2 2}.select-none{-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}.resize{resize:both}.appearance-none{-webkit-appearance:none;-moz-appearance:none;appearance:none}.grid-flow-row{grid-auto-flow:row}.grid-cols-1{grid-template-columns:repeat(1,minmax(0,1fr))}.flex-col{flex-direction:column}.items-center{align-items:center}.items-stretch{align-items:stretch}.justify-start{justify-content:flex-start}.justify-end{justify-content:flex-end}.justify-center{justify-content:center}.justify-between{justify-content:space-between}.gap-2{gap:.5rem}.divide-y>:not([hidden])~:not([hidden]){--tw-divide-y-reverse:0;border-top-width:calc(1px * calc(1 - var(--tw-divide-y-reverse)));border-bottom-width:calc(1px * var(--tw-divide-y-reverse))}.divide-y-2>:not([hidden])~:not([hidden]){--tw-divide-y-reverse:0;border-top-width:calc(2px * calc(1 - var(--tw-divide-y-reverse)));border-bottom-width:calc(2px * var(--tw-divide-y-reverse))}.overflow-hidden{overflow:hidden}.overflow-scroll{overflow:scroll}.overflow-y-scroll{overflow-y:scroll}.overscroll-none{-ms-scroll-chaining:none;overscroll-behavior:none}.whitespace-pre{white-space:pre}.whitespace-pre-wrap{white-space:pre-wrap}.border{border-width:1px}.border-bg0{--tw-border-opacity:1;border-color:rgb(0 0 0/var(--tw-border-opacity))}.border-black{--tw-border-opacity:1;border-color:rgb(0 0 0/var(--tw-border-opacity))}.bg-bg2{--tw-bg-opacity:1;background-color:rgb(43 44 43/var(--tw-bg-opacity))}.bg-bg0{--tw-bg-opacity:1;background-color:rgb(0 0 0/var(--tw-bg-opacity))}.bg-bg3{--tw-bg-opacity:1;background-color:rgb(53 56 54/var(--tw-bg-opacity))}.p-2{padding:.5rem}.p-4{padding:1rem}.p-8{padding:2rem}.px-2{padding-left:.5rem;padding-right:.5rem}.pb-4{padding-bottom:1rem}.pt-8{padding-top:2rem}.pb-2{padding-bottom:.5rem}.pl-2{padding-left:.5rem}.pl-4{padding-left:1rem}.text-center{text-align:center}.text-right{text-align:right}.font-mono{font-family:JetBrains Mono,ui-monospace,SFMono-Regular,Menlo,Monaco,Consolas,liberation mono,courier new,monospace}.text-lg{font-size:1.125rem;line-height:1.75rem}.text-sm{font-size:.875rem;line-height:1.25rem}.text-xs{font-size:.75rem;line-height:1rem}.italic{font-style:italic}.text-fg1{--tw-text-opacity:1;color:rgb(249 250 249/var(--tw-text-opacity))}.text-red-600{--tw-text-opacity:1;color:rgb(220 38 38/var(--tw-text-opacity))}.text-white{--tw-text-opacity:1;color:rgb(255 255 255/var(--tw-text-opacity))}.text-fg2{--tw-text-opacity:1;color:rgb(253 246 227/var(--tw-text-opacity))}.underline{-webkit-text-decoration-line:underline;text-decoration-line:underline}.outline-none{outline:2px solid transparent;outline-offset:2px}.outline{outline-style:solid}.outline-1{outline-width:1px}.grayscale{--tw-grayscale:grayscale(100%);filter:var(--tw-blur)var(--tw-brightness)var(--tw-contrast)var(--tw-grayscale)var(--tw-hue-rotate)var(--tw-invert)var(--tw-saturate)var(--tw-sepia)var(--tw-drop-shadow)}.filter{filter:var(--tw-blur)var(--tw-brightness)var(--tw-contrast)var(--tw-grayscale)var(--tw-hue-rotate)var(--tw-invert)var(--tw-saturate)var(--tw-sepia)var(--tw-drop-shadow)}.prose pre>code{white-space:pre-wrap!important}.prose p code{padding-left:.5rem;padding-right:.5rem}.prose p code::before{content:""}.prose p code::after{content:""}.prose img{width:100%}@font-face{font-family:jetbrains mono;src:url(https://cdn.jsdelivr.net/gh/JetBrains/JetBrainsMono@2.242/fonts/webfonts/JetBrainsMono-Regular.woff2)format("woff2");font-weight:400;font-style:normal;font-display:swap}@font-face{font-family:jetbrains mono;src:url(https://cdn.jsdelivr.net/gh/JetBrains/JetBrainsMono@2.242/fonts/webfonts/JetBrainsMono-Medium.woff2)format("woff2");font-weight:500;font-style:normal;font-display:swap}@font-face{font-family:jetbrains mono;src:url(https://cdn.jsdelivr.net/gh/JetBrains/JetBrainsMono@2.242/fonts/webfonts/JetBrainsMono-SemiBold.woff2)format("woff2");font-weight:600;font-style:normal;font-display:swap}@font-face{font-family:jetbrains mono;src:url(https://cdn.jsdelivr.net/gh/JetBrains/JetBrainsMono@2.242/fonts/webfonts/JetBrainsMono-Bold.woff2)format("woff2");font-weight:700;font-style:normal;font-display:swap}@font-face{font-family:jetbrains mono;src:url(https://cdn.jsdelivr.net/gh/JetBrains/JetBrainsMono@2.242/fonts/webfonts/JetBrainsMono-ExtraBold.woff2)format("woff2");font-weight:800;font-style:normal;font-display:swap}@font-face{font-family:jetbrains mono;src:url(https://cdn.jsdelivr.net/gh/JetBrains/JetBrainsMono@2.242/fonts/webfonts/JetBrainsMono-Light.woff2)format("woff2");font-weight:300;font-style:normal;font-display:swap}@font-face{font-family:jetbrains mono;src:url(https://cdn.jsdelivr.net/gh/JetBrains/JetBrainsMono@2.242/fonts/webfonts/JetBrainsMono-ExtraLight.woff2)format("woff2");font-weight:200;font-style:normal;font-display:swap}@font-face{font-family:jetbrains mono;src:url(https://cdn.jsdelivr.net/gh/JetBrains/JetBrainsMono@2.242/fonts/webfonts/JetBrainsMono-Thin.woff2)format("woff2");font-weight:100;font-style:normal;font-display:swap}.hover\:bg-bg4:hover{--tw-bg-opacity:1;background-color:rgb(68 68 68/var(--tw-bg-opacity))}.focus\:outline-black:focus{outline-color:#000}@media(min-width:640px){.sm\:grid-cols-2{grid-template-columns:repeat(2,minmax(0,1fr))}}@media(min-width:768px){.md\:grid-cols-3{grid-template-columns:repeat(3,minmax(0,1fr))}}@media(min-width:1024px){.lg\:grid-cols-4{grid-template-columns:repeat(4,minmax(0,1fr))}}@media(min-width:1280px){.xl\:grid-cols-5{grid-template-columns:repeat(5,minmax(0,1fr))}}</style></head><body class="bg-bg2 overscroll-none"><main class="w-full h-full p-2 font-mono flex align-center overscroll-none"><div class="w-full h-full p-2 font-mono flex flex-col items-center divide-y-2 select-none text-fg1 overscroll-none"><div class="flex-0 prose-lg max-w-none pb-4"><h1>MarkBot</h1></div><article class="flex-1 prose max-w-full pt-8 text-fg1"><p>I&rsquo;ve always had a love for large datasets and the interesting things one can do with them, so when my roommate sent me his entire Google Hangouts and Facebook chat history, I knew what I had to do.</p><h1 id=enter-markbot>Enter MarkBot</h1><h2 id=goals>Goals</h2><p>In the past, I&rsquo;ve implemented plenty of basic character-level RNNs and trained them on all sorts of text corpora, but this time I wanted to try something different, and utilize word embeddings to try and produce a more coherent Mark-based chatbot. My thought process goes like so:</p><ul><li>Word embeddings will allow the network to learn more abstract concepts beyond basic spelling and grammar, as it will not have to essentially memorize the spelling of each word in the English language (or rather, Mark&rsquo;s unique interpretation of the English language)</li><li>Word embeddings also have the advantage that similar words will lie close to each other in the vector space, so a conversational model will likely be more coherent, as it is predicting based on <strong>meaning</strong> rather than on <strong>spelling</strong></li></ul><h2 id=libraries>Libraries</h2><p>I&rsquo;ve used Python for the sake of brevity and ease of use, here are some of the libraries I&rsquo;ve used:</p><ul><li><a href=https://github.com/tqdm/tqdm>tqdm</a> - A great progress-bar library for the tedious <em>for</em> loops one often encounters when dealing with large datasets</li><li><a href=https://github.com/fchollet/keras>keras</a> - An easy-to-use deep learning tool that can sit on top of several of the most popular Python tensor mathematics libraries (I used Theano for my backend)</li><li><a href=https://github.com/nltk/nltk>nltk</a> - A handy collection of NLP tools</li><li><a href=https://github.com/maciejkula/glove-python>python-glove</a> - I have chosen GloVe over word2vec because of some slight and entirely unsubstantiated performance benefits I read about on the Internet somewhere<sup>[<strong>citation needed</strong>]</sup></li></ul><h2 id=the-project>The Project</h2><h3 id=parsing>Parsing</h3><p>In addition to Mark&rsquo;s Google Hangouts dump, I regularly back up my entire Facebook chat history, which I incorporated.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=font-weight:700;font-style:italic>print</span>(<span style=color:#666;font-style:italic>&#34;Loading datasets&#34;</span>)
</span></span><span style=display:flex><span><span style=font-weight:700;text-decoration:underline>with</span> <span style=font-weight:700;font-style:italic>open</span>(<span style=color:#666;font-style:italic>&#34;/home/will/Desktop/Python/Facebook.pkl&#34;</span>,<span style=color:#666;font-style:italic>&#34;rb&#34;</span>) <span style=font-weight:700;text-decoration:underline>as</span> f: facebook = pickle.loads(f.read())
</span></span><span style=display:flex><span><span style=font-weight:700;text-decoration:underline>with</span> <span style=font-weight:700;font-style:italic>open</span>(<span style=color:#666;font-style:italic>&#34;/home/will/Desktop/mark_hangouts.json&#34;</span>, <span style=color:#666;font-style:italic>&#34;r&#34;</span>) <span style=font-weight:700;text-decoration:underline>as</span> f: hangouts = json.loads(f.read())
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>sentences = []
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=font-weight:700;font-style:italic>print</span>(<span style=color:#666;font-style:italic>&#34;Parsing Facebook data&#34;</span>)
</span></span><span style=display:flex><span>mark_uid = <span style=color:#666;font-style:italic>&#39;1750122188&#39;</span>
</span></span><span style=display:flex><span><span style=font-weight:700;text-decoration:underline>for</span> t, ms <span style=font-weight:700>in</span> tqdm(facebook[<span style=color:#666;font-style:italic>&#34;messages&#34;</span>].items()):
</span></span><span style=display:flex><span>    ms = ms[::-1]
</span></span><span style=display:flex><span>    <span style=font-weight:700;text-decoration:underline>for</span> idx, m <span style=font-weight:700>in</span> <span style=font-weight:700;font-style:italic>enumerate</span>(ms):
</span></span><span style=display:flex><span>        <span style=font-weight:700;text-decoration:underline>if</span> <span style=font-weight:700>not</span> m.text:
</span></span><span style=display:flex><span>            <span style=font-weight:700;text-decoration:underline>continue</span>
</span></span><span style=display:flex><span>        text = m.text
</span></span><span style=display:flex><span>        <span style=font-weight:700;text-decoration:underline>if</span> <span style=font-weight:700>not</span> text:
</span></span><span style=display:flex><span>            <span style=font-weight:700;text-decoration:underline>continue</span>
</span></span><span style=display:flex><span>        is_mark = m.author == mark_uid
</span></span><span style=display:flex><span>        sentences.append((text, is_mark))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=font-weight:700;font-style:italic>print</span>(<span style=color:#666;font-style:italic>&#34;Parsing Hangouts data&#34;</span>)
</span></span><span style=display:flex><span>mark_chatid = <span style=color:#666;font-style:italic>&#34;116404337397755400265&#34;</span>
</span></span><span style=display:flex><span><span style=font-weight:700;text-decoration:underline>for</span> t <span style=font-weight:700>in</span> tqdm(hangouts[<span style=color:#666;font-style:italic>&#34;conversation_state&#34;</span>]):
</span></span><span style=display:flex><span>    ms = t[<span style=color:#666;font-style:italic>&#34;conversation_state&#34;</span>][<span style=color:#666;font-style:italic>&#34;event&#34;</span>]
</span></span><span style=display:flex><span>    <span style=font-weight:700;text-decoration:underline>for</span> idx, m <span style=font-weight:700>in</span> <span style=font-weight:700;font-style:italic>enumerate</span>(ms):
</span></span><span style=display:flex><span>        <span style=font-weight:700;text-decoration:underline>if</span> <span style=color:#666;font-style:italic>&#34;chat_message&#34;</span> <span style=font-weight:700>not</span> <span style=font-weight:700>in</span> m <span style=font-weight:700>or</span> <span style=color:#666;font-style:italic>&#34;segment&#34;</span> <span style=font-weight:700>not</span> <span style=font-weight:700>in</span> m[<span style=color:#666;font-style:italic>&#34;chat_message&#34;</span>][<span style=color:#666;font-style:italic>&#34;message_content&#34;</span>]:
</span></span><span style=display:flex><span>            <span style=font-weight:700;text-decoration:underline>continue</span>
</span></span><span style=display:flex><span>        text = <span style=color:#666;font-style:italic>&#34;</span><span style=color:#666;font-style:italic>\n</span><span style=color:#666;font-style:italic>&#34;</span>.join(segment[<span style=color:#666;font-style:italic>&#34;text&#34;</span>] <span style=font-weight:700;text-decoration:underline>for</span> segment <span style=font-weight:700>in</span> m[<span style=color:#666;font-style:italic>&#34;chat_message&#34;</span>][<span style=color:#666;font-style:italic>&#34;message_content&#34;</span>][<span style=color:#666;font-style:italic>&#34;segment&#34;</span>] <span style=font-weight:700;text-decoration:underline>if</span> <span style=color:#666;font-style:italic>&#34;text&#34;</span> <span style=font-weight:700>in</span> segment)
</span></span><span style=display:flex><span>        <span style=font-weight:700;text-decoration:underline>if</span> <span style=font-weight:700>not</span> text:
</span></span><span style=display:flex><span>            <span style=font-weight:700;text-decoration:underline>continue</span>
</span></span><span style=display:flex><span>        is_mark = m[<span style=color:#666;font-style:italic>&#34;sender_id&#34;</span>][<span style=color:#666;font-style:italic>&#34;chat_id&#34;</span>] == mark_chatid
</span></span><span style=display:flex><span>        sentences.append((text, is_mark))
</span></span><span style=display:flex><span><span style=font-weight:700;font-style:italic>print</span>(<span style=color:#666;font-style:italic>&#34;Loaded&#34;</span>, <span style=font-weight:700;font-style:italic>len</span>(sentences), <span style=color:#666;font-style:italic>&#34;sentences&#34;</span>)
</span></span><span style=display:flex><span><span style=font-weight:700;text-decoration:underline>del</span> facebook, hangouts
</span></span></code></pre></div><p>TL;DR: Parsing Google Hangouts JSON is a pain and a half. Also, I tend to throw PEP out the window when it comes to single-use projects like this.</p><p>What this code does:</p><ul><li>Combines subsequent messages from one user into a single message</li><li>Prepares a list of sentences with both text data and an <em>is_mark</em> boolean, which is fairly self-explanatory. This allows us to build (question, response) tuples later</li><li>Delete references to things we don&rsquo;t need after the fact</li></ul><p><b></b></p><pre><code>&gt;&gt; sentences
[
 (&quot;Hey mark what's up?&quot;, False),
 (&quot;Not much, how about you?&quot;, True),
 ...
]
</code></pre><h3 id=tokenizing>Tokenizing</h3><p>Now let&rsquo;s separate messages into their constituent words, or <em>tokens</em></p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=font-weight:700;font-style:italic>print</span>(<span style=color:#666;font-style:italic>&#34;Tokenizing messages&#34;</span>)
</span></span><span style=display:flex><span><span style=font-weight:700;text-decoration:underline>import</span> <span style=color:#666;font-weight:700;font-style:italic>collections</span>
</span></span><span style=display:flex><span><span style=font-weight:700;text-decoration:underline>import</span> <span style=color:#666;font-weight:700;font-style:italic>nltk</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>sentences = [(s[0].lower(), s[1]) <span style=font-weight:700;text-decoration:underline>for</span> s <span style=font-weight:700>in</span> sentences]
</span></span><span style=display:flex><span>vocab = collections.defaultdict(<span style=font-weight:700;text-decoration:underline>lambda</span>: 0)
</span></span><span style=display:flex><span><span style=font-weight:700;text-decoration:underline>for</span> idx, s <span style=font-weight:700>in</span> <span style=font-weight:700;font-style:italic>enumerate</span>(tqdm(sentences)):
</span></span><span style=display:flex><span>    words = nltk.word_tokenize(s[0])
</span></span><span style=display:flex><span>    sentences[idx] = (words, s[1])
</span></span><span style=display:flex><span>    <span style=font-weight:700;text-decoration:underline>for</span> w <span style=font-weight:700>in</span> words:
</span></span><span style=display:flex><span>        vocab[w] += 1
</span></span><span style=display:flex><span>vocab = <span style=font-weight:700;font-style:italic>dict</span>(vocab)
</span></span><span style=display:flex><span><span style=font-weight:700;font-style:italic>print</span>(<span style=color:#666;font-style:italic>&#34;</span><span style=color:#666;font-style:italic>{}</span><span style=color:#666;font-style:italic> unfiltered vocabulary words&#34;</span>.format(<span style=font-weight:700;font-style:italic>len</span>(vocab)))
</span></span></code></pre></div><p>What this code does:</p><ul><li>Lower-cases and tokenizes all the messages</li><li>Counts word frequencies</li></ul><p>By lower-casing the messages, we reduce the vocabulary size, as otherwise &ldquo;Car&rdquo; and &ldquo;car&rdquo; would be interpreted as different words</p><p><b></b></p><pre><code>&gt;&gt; sentences
[
 ([&quot;hey&quot;, &quot;mark&quot;, &quot;what&quot;, &quot;'s&quot;, &quot;up&quot;, &quot;?&quot;], False),
 ([&quot;not&quot;, &quot;much&quot;, &quot;,&quot;, &quot;how&quot;, &quot;about&quot;, &quot;you&quot;, &quot;?&quot;], True),
 ...
]
</code></pre><h3 id=query-response>Query, Response</h3><p>Now I need to form (query, response) tuples if this is going to work as a conversational model. This is when we&rsquo;ll use the <em>is_mark</em> bool.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=font-weight:700;font-style:italic>print</span>(<span style=color:#666;font-style:italic>&#34;Assembling response training data&#34;</span>)
</span></span><span style=display:flex><span><span style=font-weight:700;text-decoration:underline>import</span> <span style=color:#666;font-weight:700;font-style:italic>random</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>BOS, EOS = <span style=color:#666;font-style:italic>&#34;</span><span style=color:#666;font-style:italic>\x00</span><span style=color:#666;font-style:italic>&#34;</span>, <span style=color:#666;font-style:italic>&#34;</span><span style=color:#666;font-style:italic>\x01</span><span style=color:#666;font-style:italic>&#34;</span>
</span></span><span style=display:flex><span>exchanges = []
</span></span><span style=display:flex><span>idxs = [idx <span style=font-weight:700;text-decoration:underline>for</span> idx, s <span style=font-weight:700>in</span> <span style=font-weight:700;font-style:italic>enumerate</span>(sentences) <span style=font-weight:700;text-decoration:underline>if</span> s[1] <span style=font-weight:700>and</span> idx &gt;= CONTEXT_LINES]
</span></span><span style=display:flex><span>sentences = [s[0] <span style=font-weight:700;text-decoration:underline>for</span> s <span style=font-weight:700>in</span> sentences]
</span></span><span style=display:flex><span><span style=color:#888;font-style:italic>#Also perform pruning to remove query/response pairs with uncommon words</span>
</span></span><span style=display:flex><span>prune = <span style=font-weight:700;font-style:italic>set</span>([w <span style=font-weight:700;text-decoration:underline>for</span> w <span style=font-weight:700>in</span> vocab <span style=font-weight:700;text-decoration:underline>if</span> vocab[w] &lt; 9])
</span></span><span style=display:flex><span><span style=font-weight:700;text-decoration:underline>for</span> idx <span style=font-weight:700>in</span> idxs:
</span></span><span style=display:flex><span>    exchange = [s <span style=font-weight:700;text-decoration:underline>for</span> s <span style=font-weight:700>in</span> sentences[idx-CONTEXT_LINES: idx+1]]
</span></span><span style=display:flex><span>    exchange[-1] = [BOS] + exchange[-1] + [EOS]
</span></span><span style=display:flex><span>    <span style=font-weight:700;text-decoration:underline>if</span> <span style=font-weight:700;font-style:italic>any</span>([w <span style=font-weight:700>in</span> prune <span style=font-weight:700;text-decoration:underline>for</span> s <span style=font-weight:700>in</span> exchange <span style=font-weight:700;text-decoration:underline>for</span> w <span style=font-weight:700>in</span> s]):
</span></span><span style=display:flex><span>        <span style=font-weight:700;text-decoration:underline>continue</span>
</span></span><span style=display:flex><span>    <span style=font-weight:700;text-decoration:underline>else</span>:
</span></span><span style=display:flex><span>        exchanges.append(exchange)
</span></span><span style=display:flex><span>sentences = [s <span style=font-weight:700;text-decoration:underline>for</span> e <span style=font-weight:700>in</span> exchanges <span style=font-weight:700;text-decoration:underline>for</span> s <span style=font-weight:700>in</span> e]
</span></span><span style=display:flex><span>exchanges = [([w <span style=font-weight:700;text-decoration:underline>for</span> s <span style=font-weight:700>in</span> e[:-1] <span style=font-weight:700;text-decoration:underline>for</span> w <span style=font-weight:700>in</span> s], e[-1]) <span style=font-weight:700;text-decoration:underline>for</span> e <span style=font-weight:700>in</span> exchanges]
</span></span><span style=display:flex><span>random.shuffle(exchanges)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#888;font-style:italic>#Now recompute our vocabulary</span>
</span></span><span style=display:flex><span>vocab = collections.defaultdict(<span style=font-weight:700;text-decoration:underline>lambda</span>: 0)
</span></span><span style=display:flex><span><span style=font-weight:700;text-decoration:underline>for</span> s <span style=font-weight:700>in</span> sentences:
</span></span><span style=display:flex><span>    <span style=font-weight:700;text-decoration:underline>for</span> w <span style=font-weight:700>in</span> s:
</span></span><span style=display:flex><span>        vocab[w] += 1
</span></span><span style=display:flex><span>vocab = <span style=font-weight:700;font-style:italic>dict</span>(vocab)
</span></span><span style=display:flex><span><span style=font-weight:700;font-style:italic>print</span>(<span style=color:#666;font-style:italic>&#34;</span><span style=color:#666;font-style:italic>{}</span><span style=color:#666;font-style:italic> query/response pairs</span><span style=color:#666;font-style:italic>\n</span><span style=color:#666;font-style:italic>{}</span><span style=color:#666;font-style:italic> vocabulary words&#34;</span>.format(<span style=font-weight:700;font-style:italic>len</span>(exchanges), <span style=font-weight:700;font-style:italic>len</span>(vocab)-2))
</span></span></code></pre></div><p>A few things are happening here:</p><ul><li>(query, response) tuples, or <strong>exchanges</strong>, are prepared</li><li>Exchanges with uncommon vocabulary are discarded. In this dataset, uncommon vocabulary mostly manifests in the form of esoteric emoji, which I don&rsquo;t particularly need this conversational model to reproduce.<br>Like I said, this data merely represents Mark&rsquo;s unique subset of the English language (or rather, the two sets intersect but also contain elements unique to themselves, as I do not believe &lsquo;😍&rsquo; is in the OED, nor have I ever heard Mark say &rsquo;lexiphanicism&rsquo;, though he is certainly prone to it)</li><li>Beginning-of-sequence and end-of-sequence markers are inserted into the response element, for use later in the neural chatbot model</li><li>Finally, the vocabulary is recomputed to account for the removal of exchanges containing uncommon vocabulary. I could have simply replaced these uncommon words with an unknown-word token, but thought it would be cleaner to not go that route.</li></ul><p>This is what the data looks like now:</p><pre><code>&gt;&gt; exchanges
[
 (
  [&quot;hey&quot;, &quot;mark&quot;, &quot;what&quot;, &quot;'s&quot;, &quot;up&quot;, &quot;?&quot;],
  [&quot;\x00&quot;, &quot;not&quot;, &quot;much&quot;, &quot;,&quot;, &quot;how&quot;, &quot;about&quot;, &quot;you&quot;, &quot;?&quot;, &quot;\x01&quot;]
 ),
 ...
]
</code></pre><h3 id=converting-to-indices>Converting to Indices</h3><p>Now let&rsquo;s turn these words into numbers!</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#888;font-style:italic>#The lower a word&#39;s lookup index, the higher its frequency in the corpus</span>
</span></span><span style=display:flex><span><span style=font-weight:700;text-decoration:underline>import</span> <span style=color:#666;font-weight:700;font-style:italic>keras</span>
</span></span><span style=display:flex><span><span style=font-weight:700;text-decoration:underline>from</span> <span style=color:#666;font-weight:700;font-style:italic>keras.preprocessing.sequence</span> <span style=font-weight:700;text-decoration:underline>import</span> pad_sequences
</span></span><span style=display:flex><span>lookup_table =      {idx: w <span style=font-weight:700;text-decoration:underline>for</span> idx, w <span style=font-weight:700>in</span> <span style=font-weight:700;font-style:italic>enumerate</span>(<span style=font-weight:700;font-style:italic>sorted</span>(vocab, key=<span style=font-weight:700;text-decoration:underline>lambda</span> w: vocab[w])[::-1])}
</span></span><span style=display:flex><span>lookup_table.update({w: idx <span style=font-weight:700;text-decoration:underline>for</span> idx, w <span style=font-weight:700>in</span> <span style=font-weight:700;font-style:italic>enumerate</span>(<span style=font-weight:700;font-style:italic>sorted</span>(vocab, key=<span style=font-weight:700;text-decoration:underline>lambda</span> w: vocab[w])[::-1])})
</span></span><span style=display:flex><span>X = np.asarray([[lookup_table[w] <span style=font-weight:700;text-decoration:underline>for</span> w <span style=font-weight:700>in</span> query] <span style=font-weight:700;text-decoration:underline>for</span> query <span style=font-weight:700>in</span> (r[0] <span style=font-weight:700;text-decoration:underline>for</span> r <span style=font-weight:700>in</span> exchanges)])
</span></span><span style=display:flex><span>Y = np.asarray([[lookup_table[w] <span style=font-weight:700;text-decoration:underline>for</span> w <span style=font-weight:700>in</span> response] <span style=font-weight:700;text-decoration:underline>for</span> response <span style=font-weight:700>in</span> (r[1] <span style=font-weight:700;text-decoration:underline>for</span> r <span style=font-weight:700>in</span> exchanges)])
</span></span><span style=display:flex><span>X = pad_sequences(X, SEQUENCE_LEN)
</span></span><span style=display:flex><span>Y = pad_sequences(Y, SEQUENCE_LEN, padding=<span style=color:#666;font-style:italic>&#34;post&#34;</span>)
</span></span></code></pre></div><ul><li><p>Firstly, the lookup table simply maps words to their indexes and back again</p></li><li><p>The X and Y variables now contain the query, response data but in index format, and padded to length</p><pre tabindex=0><code>&gt;&gt; X 
array([
       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
          0,   0,   0,   0,   0, 262,  73,  28,  19,  44,   4
       ],
       ...
      ], dtype=int32 )

&gt;&gt; Y
array([
       [  1,  31, 107,  10,  54,  76,   9,   4,   0,   0,   0,   0,   0,
          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0
       ],
       ...
      ], dtype=int32 )
</code></pre></li></ul><p>We are still dealing with words here though, the integers in X and Y do not encode any meaning. That comes next.</p><p>As you can see, queries are padded at the <strong>beginning</strong> and responses are padded at the <strong>end</strong>, so you can see that we&rsquo;re going to be feeding it a certain amount of context and expecting some variable amount of response, stopping when we hit the 0&rsquo;s (which represent end-of-sequence markers)</p><h3 id=vectorization>Vectorization</h3><p>The fun begins! Now we are going to train with the GloVe algorithm to embed our words into a high dimensional space that encodes semantic content and relationships.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=font-weight:700;font-style:italic>print</span>(<span style=color:#666;font-style:italic>&#34;Building vocabulary model&#34;</span>)
</span></span><span style=display:flex><span><span style=font-weight:700;text-decoration:underline>import</span> <span style=color:#666;font-weight:700;font-style:italic>glove</span>
</span></span><span style=display:flex><span><span style=font-weight:700;text-decoration:underline>import</span> <span style=color:#666;font-weight:700;font-style:italic>multiprocessing</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>corpus = glove.Corpus()
</span></span><span style=display:flex><span>corpus.fit(sentences, window=10)
</span></span><span style=display:flex><span>glove_model = glove.Glove(no_components=WORD_EMBEDDING_COMPONENTS, learning_rate=0.05)
</span></span><span style=display:flex><span>glove_model.fit(corpus.matrix, epochs=30, no_threads=multiprocessing.cpu_count(), verbose=<span style=font-weight:700;text-decoration:underline>True</span>)
</span></span><span style=display:flex><span><span style=font-weight:700;text-decoration:underline>del</span> sentences    
</span></span></code></pre></div><p>The model has now been trained on the available sentences and we can do useful things with the resulting word embeddings! But we&rsquo;re not going to. Instead we&rsquo;re going to virtualize my roommate.</p><h3 id=building-the-neural-model>Building the neural model</h3><p>We&rsquo;re into the thick of it now. I have adapted a model I found <a href=https://github.com/oswaldoludwig/Seq2seq-Chatbot-for-Keras>here</a> that implements a sequence-to-sequence architecture.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=font-weight:700;font-style:italic>print</span>(<span style=color:#666;font-style:italic>&#34;Building neural chatbot model&#34;</span>)
</span></span><span style=display:flex><span>optimizer = keras.optimizers.Adam(lr=0.001) 
</span></span><span style=display:flex><span>input_context = keras.layers.Input(shape=(SEQUENCE_LEN,), dtype=<span style=color:#666;font-style:italic>&#39;int32&#39;</span>, name=<span style=color:#666;font-style:italic>&#39;input_context&#39;</span>)
</span></span><span style=display:flex><span>inputanswer_chunkanswer = keras.layers.Input(shape=(SEQUENCE_LEN,), dtype=<span style=color:#666;font-style:italic>&#39;int32&#39;</span>, name=<span style=color:#666;font-style:italic>&#39;inputanswer_chunkanswer&#39;</span>)
</span></span><span style=display:flex><span>LSTM_encoder = keras.layers.LSTM(SENTENCE_EMBEDDING_SIZE, kernel_initializer=<span style=color:#666;font-style:italic>&#39;lecun_uniform&#39;</span>)
</span></span><span style=display:flex><span>LSTM_decoder = keras.layers.LSTM(SENTENCE_EMBEDDING_SIZE, kernel_initializer=<span style=color:#666;font-style:italic>&#39;lecun_uniform&#39;</span>)
</span></span><span style=display:flex><span>Shared_Embedding = keras.layers.Embedding(output_dim=glove_model.no_components, input_dim=<span style=font-weight:700;font-style:italic>len</span>(vocab), weights=[glove_model.word_vectors.copy()], input_length=SEQUENCE_LEN)
</span></span><span style=display:flex><span>word_embedding_context = Shared_Embedding(input_context)
</span></span><span style=display:flex><span>context_embedding = LSTM_encoder(word_embedding_context)
</span></span><span style=display:flex><span>word_embeddinganswer_chunknswer = Shared_Embedding(inputanswer_chunkanswer)
</span></span><span style=display:flex><span>answer_embedding = LSTM_decoder(word_embeddinganswer_chunknswer)
</span></span><span style=display:flex><span>merge_layer = keras.layers.concatenate([context_embedding, answer_embedding], axis=1)
</span></span><span style=display:flex><span><span style=color:#888;font-style:italic>#We can change the number of neurons in this Dense layer</span>
</span></span><span style=display:flex><span>out = keras.layers.Dense(<span style=font-weight:700;font-style:italic>len</span>(vocab)//2, activation=<span style=color:#666;font-style:italic>&#34;relu&#34;</span>)(merge_layer)
</span></span><span style=display:flex><span>out = keras.layers.Dense(<span style=font-weight:700;font-style:italic>len</span>(vocab), activation=<span style=color:#666;font-style:italic>&#34;softmax&#34;</span>)(out)
</span></span><span style=display:flex><span>keras_model = keras.models.Model(inputs=[input_context, inputanswer_chunkanswer], outputs=[out])
</span></span><span style=display:flex><span>keras_model.compile(loss=<span style=color:#666;font-style:italic>&#39;categorical_crossentropy&#39;</span>, optimizer=optimizer)
</span></span><span style=display:flex><span><span style=font-weight:700;text-decoration:underline>del</span> glove_model
</span></span></code></pre></div><p>This graphic gives the general idea - we&rsquo;re feeding in both the query and the response so far into the network to get the next word in the response</p><p><img src=https://raw.githubusercontent.com/oswaldoludwig/Seq2seq-Chatbot-for-Keras/master/model_graph.png alt="alt text" title="[again, not mine]"></p><p>At this point, a word looks like this:</p><pre><code>array([ 0.06541251, -0.60211362,  0.19668873,  0.29943736, -0.04908019,
       -0.25159775, -0.48105127,  0.18929953, -0.35931338,  0.21183521,
        0.32535496,  0.24972419,  0.09867496,  0.28443115,  0.50898685,
       -0.04326349,  0.10213821, -0.14590992,  0.33488042, -0.14846319,
       -0.1409087 , -0.19627512,  0.27064866,  0.30814872,  0.44088078,
       -0.39964452,  0.2399658 , -0.13583342,  0.02606956,  0.30684316,
       -0.12864215,  0.10482805,  0.28297046,  0.07438818, -0.29477953,
        0.10404356, -0.32852114, -0.09728817, -0.20320519,  0.18048016,
        0.37241584,  0.15531563,  0.16767227, -0.02549689,  0.24815189,
       -0.11939143, -0.29207007,  0.26256024,  0.41336682,  0.23037289,
       -0.23548023, -0.16578295,  0.00896039,  0.20869134, -0.14342295,
       -0.23251574,  0.40983124,  0.19986974,  0.16265753, -0.18266805,
       -0.1975813 , -0.33908948,  0.18949206, -0.11479648,  0.35812903,
       -0.16103889,  0.14967493,  0.30761199,  0.31005254, -0.04312443,
       -0.04538435, -0.21032511, -0.18207666, -0.27363791, -0.00809431,
       -0.16818864, -0.12332384, -0.2938934 , -0.2526425 , -0.23394612,
        0.53925823,  0.31640659,  0.62309644,  0.28594853, -0.39425204,
       -0.1976824 , -0.14072738, -0.51376977,  0.18433504, -0.04015992,
        0.26220768, -0.13670408,  0.15177998,  0.27146796,  0.03903609,
       -0.06253815,  0.42974848,  0.30019652, -0.05678444,  0.18900654])
</code></pre><h3 id=training-the-neural-model>Training the neural model</h3><p>We have to do a lot of chunking here because the size of the arrays we&rsquo;re creating is so large that I keep running out of RAM</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>chunk_size = X.shape[0] // NUM_CHUNKS
</span></span><span style=display:flex><span><span style=font-weight:700;text-decoration:underline>for</span> epoch <span style=font-weight:700>in</span> <span style=font-weight:700;font-style:italic>range</span>(EPOCHS):
</span></span><span style=display:flex><span>    <span style=font-weight:700;font-style:italic>print</span>(<span style=color:#666;font-style:italic>&#34;Training epoch </span><span style=color:#666;font-style:italic>{}</span><span style=color:#666;font-style:italic>/</span><span style=color:#666;font-style:italic>{}</span><span style=color:#666;font-style:italic>&#34;</span>.format(epoch+1, EPOCHS))
</span></span><span style=display:flex><span>    <span style=font-weight:700;text-decoration:underline>try</span>:
</span></span><span style=display:flex><span>        <span style=font-weight:700;text-decoration:underline>for</span> chunk_idx, chunk <span style=font-weight:700>in</span> <span style=font-weight:700;font-style:italic>enumerate</span>(<span style=font-weight:700;font-style:italic>range</span>(0, <span style=font-weight:700;font-style:italic>len</span>(X), chunk_size)):
</span></span><span style=display:flex><span>            <span style=font-weight:700;font-style:italic>print</span>(<span style=color:#666;font-style:italic>&#34;Training chunk </span><span style=color:#666;font-style:italic>{}</span><span style=color:#666;font-style:italic>&#34;</span>.format(chunk_idx+1))
</span></span><span style=display:flex><span>            X_chunk = X[chunk: chunk+chunk_size]
</span></span><span style=display:flex><span>            Y_chunk = Y[chunk: chunk+chunk_size]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>            total_unpadded_len = 0
</span></span><span style=display:flex><span>            <span style=font-weight:700;text-decoration:underline>for</span> i, y <span style=font-weight:700>in</span> <span style=font-weight:700;font-style:italic>enumerate</span>(Y_chunk):
</span></span><span style=display:flex><span>                unpadded_len = np.where(y == lookup_table[EOS])[0][0]
</span></span><span style=display:flex><span>                total_unpadded_len += unpadded_len + 1
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>            context_chunk = np.zeros((total_unpadded_len, SEQUENCE_LEN))
</span></span><span style=display:flex><span>            answer_chunk = np.zeros((total_unpadded_len, SEQUENCE_LEN))
</span></span><span style=display:flex><span>            next_word_chunk = np.zeros((total_unpadded_len, <span style=font-weight:700;font-style:italic>len</span>(vocab)))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>            count = 0
</span></span><span style=display:flex><span>            <span style=font-weight:700;text-decoration:underline>for</span> i, y <span style=font-weight:700>in</span> <span style=font-weight:700;font-style:italic>enumerate</span>(Y_chunk):
</span></span><span style=display:flex><span>                <span style=color:#888;font-style:italic>#Prepare one-hot encoding</span>
</span></span><span style=display:flex><span>                answer_partial = np.zeros((1, SEQUENCE_LEN))
</span></span><span style=display:flex><span>                limit = np.where(y == lookup_table[EOS])[0][0]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>                <span style=font-weight:700;text-decoration:underline>for</span> symbol_idx <span style=font-weight:700>in</span> <span style=font-weight:700;font-style:italic>range</span>(1, limit+1):
</span></span><span style=display:flex><span>                    one_hot = np.zeros((1, <span style=font-weight:700;font-style:italic>len</span>(vocab)))
</span></span><span style=display:flex><span>                    one_hot[0, y[symbol_idx]] = 1
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>                    answer_partial[0, -symbol_idx:] = y[0: symbol_idx]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>                    context_chunk[count, :] = X_chunk[i: i+1]
</span></span><span style=display:flex><span>                    answer_chunk[count, :] = answer_partial
</span></span><span style=display:flex><span>                    next_word_chunk[count, :] = one_hot
</span></span><span style=display:flex><span>                    count += 1
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>            keras_model.fit([context_chunk, answer_chunk], next_word_chunk, batch_size=BATCH_SIZE, epochs=1)
</span></span><span style=display:flex><span>            <span style=font-weight:700;text-decoration:underline>del</span> context_chunk, answer_chunk, next_word_chunk
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=font-weight:700;text-decoration:underline>except</span> KeyboardInterrupt:
</span></span><span style=display:flex><span>        <span style=font-weight:700;text-decoration:underline>break</span>
</span></span></code></pre></div><p>This code is training the neural network bit by bit so it learns how to converse, given a query and a partial answer.</p><p>For example, given the vector arrays for the query string &ldquo;what is your name&rdquo; and partial answer string &ldquo;my name is&rdquo;, it would output a vector that is very close to the vector for the word &ldquo;mark&rdquo; (assuming Mark has answered this question often enough for it to learn that response)</p><p>Now all we have to do is test it!</p><h3 id=testing-the-chatbot>Testing the chatbot!</h3><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=font-weight:700;font-style:italic>print</span>(<span style=color:#666;font-style:italic>&#34;Testing chatbot&#34;</span>)
</span></span><span style=display:flex><span><span style=color:#888;font-style:italic>#Helper function to sample an index from an array of probabilities</span>
</span></span><span style=display:flex><span><span style=font-weight:700;text-decoration:underline>def</span> <span style=color:#666;font-weight:700;font-style:italic>sample</span>(preds, temperature=1.0):
</span></span><span style=display:flex><span>    preds = np.asarray(preds).astype(<span style=color:#666;font-style:italic>&#39;float64&#39;</span>)
</span></span><span style=display:flex><span>    preds = np.log(preds) / temperature
</span></span><span style=display:flex><span>    exp_preds = np.exp(preds)
</span></span><span style=display:flex><span>    preds = exp_preds / np.sum(exp_preds)
</span></span><span style=display:flex><span>    probas = np.random.multinomial(1, preds, 1)
</span></span><span style=display:flex><span>    <span style=font-weight:700;text-decoration:underline>return</span> np.argmax(probas)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=font-weight:700;text-decoration:underline>def</span> <span style=color:#666;font-weight:700;font-style:italic>ask</span>(query, temperature=0.3):
</span></span><span style=display:flex><span>    <span style=font-weight:700;font-style:italic>print</span>(query)
</span></span><span style=display:flex><span>    query = np.asarray([lookup_table[w] <span style=font-weight:700;text-decoration:underline>for</span> w <span style=font-weight:700>in</span> nltk.word_tokenize(query.lower())])
</span></span><span style=display:flex><span>    <span style=color:#888;font-style:italic>#Pad to length of query sentence</span>
</span></span><span style=display:flex><span>    query = np.pad(query[-SEQUENCE_LEN:], pad_width=(SEQUENCE_LEN-<span style=font-weight:700;font-style:italic>len</span>(query), 0), mode=<span style=color:#666;font-style:italic>&#34;constant&#34;</span>)
</span></span><span style=display:flex><span>    query = np.asarray([query])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    response = np.zeros((1, SEQUENCE_LEN))
</span></span><span style=display:flex><span>    <span style=color:#888;font-style:italic>#Insert BOS marker</span>
</span></span><span style=display:flex><span>    response[0, -1] = lookup_table[BOS]
</span></span><span style=display:flex><span>    <span style=font-weight:700;text-decoration:underline>for</span> k <span style=font-weight:700>in</span> <span style=font-weight:700;font-style:italic>range</span>(SEQUENCE_LEN - 1):
</span></span><span style=display:flex><span>        pred = keras_model.predict([query, response])[0]
</span></span><span style=display:flex><span>        next_token = sample(pred, temperature)
</span></span><span style=display:flex><span>        <span style=color:#888;font-style:italic>#Shift partial answer over one</span>
</span></span><span style=display:flex><span>        response[0, :-1] = response[0, 1:]
</span></span><span style=display:flex><span>        response[0, -1] = next_token
</span></span><span style=display:flex><span>        <span style=color:#888;font-style:italic>#Is the model telling us to end the sentence?</span>
</span></span><span style=display:flex><span>        <span style=font-weight:700;text-decoration:underline>if</span> next_token == lookup_table[EOS]:
</span></span><span style=display:flex><span>            <span style=font-weight:700;text-decoration:underline>break</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    response = <span style=color:#666;font-style:italic>&#34; &#34;</span>.join([lookup_table[<span style=font-weight:700;font-style:italic>int</span>(token)] <span style=font-weight:700;text-decoration:underline>for</span> token <span style=font-weight:700>in</span> response[0] <span style=font-weight:700;text-decoration:underline>if</span> token <span style=font-weight:700>not</span> <span style=font-weight:700>in</span> (lookup_table[BOS], lookup_table[EOS])])
</span></span><span style=display:flex><span>    <span style=font-weight:700;font-style:italic>print</span>(response)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>ask(<span style=color:#666;font-style:italic>&#34;What is the meaning of life?&#34;</span>)
</span></span></code></pre></div><p>The sample function allows us to introduce some uncertainty into the reply, so it&rsquo;s not the same every time.</p><p>And the response, after the default 10 epochs of training?</p><pre><code>&gt;&gt; ask(&quot;What is the meaning of life?&quot;)
i do n't know what i 'm doing
</code></pre><p>Overfitting aside, that sure sounds like Mark.</p><p><em>YMMV</em></p></article></div></main></body></html>